{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9777f788",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bca954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Running on device: cpu\n",
      "üìÅ Results directory created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Add the src directory to Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "# Import Custom Modules\n",
    "import pre_processing as pp\n",
    "from ResNet18 import ResNet18_Grayscale\n",
    "from engine import train_model, validate, train_one_epoch\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Running on device: {device}\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results/models', exist_ok=True)\n",
    "print(\"üìÅ Results directory created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947d576",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4599bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Data...\n",
      "Train Data: (2099, 128, 128)\n",
      "Val Data:   (459, 128, 128)\n",
      "Test Data:  (66, 128, 128)\n",
      "‚úÖ Data loaded and helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Loading Data...\")\n",
    "\n",
    "# Load and split data\n",
    "X, Y, Z, proba = pp.get_data()\n",
    "\n",
    "# Standard Split (Train/Val/Test)\n",
    "X_train, Y_train, Z_train, X_val, Y_val, Z_val, X_test, Y_test, Z_test = pp.split_data(X, Y, Z)\n",
    "\n",
    "print(f\"Train Data: {X_train.shape}\")\n",
    "print(f\"Val Data:   {X_val.shape}\")\n",
    "print(f\"Test Data:  {X_test.shape}\")\n",
    "\n",
    "# Helper to create DataLoaders\n",
    "def get_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    if X.ndim == 3: \n",
    "        X = np.expand_dims(X, axis=1)\n",
    "    # Convert to tensor\n",
    "    tensor_x = torch.Tensor(X)\n",
    "    tensor_y = torch.Tensor(y)\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "print(\"‚úÖ Data loaded and helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334cb02",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f9aa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç STARTING HYPERPARAMETER TUNING (on Baseline Data)...\n",
      "  Testing LR=0.001, Batch=16...Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/kiansorosh/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:08<00:00, 5.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu for 5 epochs...\n",
      "Ep 1: TrLoss 0.5602 | Val F1 0.6245 | ValLoss 0.6473 | LR 0.001000\n",
      "Ep 2: TrLoss 0.4370 | Val F1 0.6788 | ValLoss 0.4028 | LR 0.001000\n",
      "Ep 3: TrLoss 0.4443 | Val F1 0.5316 | ValLoss 0.5884 | LR 0.001000\n",
      "Ep 4: TrLoss 0.4161 | Val F1 0.6131 | ValLoss 0.3712 | LR 0.001000\n",
      "Ep 5: TrLoss 0.3891 | Val F1 0.6977 | ValLoss 0.4326 | LR 0.001000\n",
      " Best F1: 0.6977\n",
      "  Testing LR=0.001, Batch=32...Training on cpu for 5 epochs...\n",
      "Ep 1: TrLoss 0.5217 | Val F1 0.4554 | ValLoss 0.5359 | LR 0.001000\n",
      "Ep 2: TrLoss 0.4377 | Val F1 0.6381 | ValLoss 0.4207 | LR 0.001000\n",
      "Ep 3: TrLoss 0.3902 | Val F1 0.6791 | ValLoss 0.3401 | LR 0.001000\n",
      "Ep 4: TrLoss 0.3403 | Val F1 0.6891 | ValLoss 0.3830 | LR 0.001000\n",
      "Ep 5: TrLoss 0.3174 | Val F1 0.7376 | ValLoss 0.3448 | LR 0.001000\n",
      " Best F1: 0.7376\n",
      "  Testing LR=0.0001, Batch=16...Training on cpu for 5 epochs...\n",
      "Ep 1: TrLoss 0.4630 | Val F1 0.5897 | ValLoss 0.4644 | LR 0.000100\n",
      "Ep 2: TrLoss 0.3275 | Val F1 0.6244 | ValLoss 0.4200 | LR 0.000100\n",
      "Ep 3: TrLoss 0.2583 | Val F1 0.6833 | ValLoss 0.5232 | LR 0.000100\n",
      "Ep 4: TrLoss 0.2001 | Val F1 0.7352 | ValLoss 0.4369 | LR 0.000100\n",
      "Ep 5: TrLoss 0.1799 | Val F1 0.7568 | ValLoss 0.4252 | LR 0.000100\n",
      " Best F1: 0.7568\n",
      "  Testing LR=0.0001, Batch=32...Training on cpu for 5 epochs...\n",
      "Ep 1: TrLoss 0.4149 | Val F1 0.7089 | ValLoss 0.3478 | LR 0.000100\n",
      "Ep 2: TrLoss 0.2477 | Val F1 0.6807 | ValLoss 0.3954 | LR 0.000100\n",
      "Ep 3: TrLoss 0.1450 | Val F1 0.7364 | ValLoss 0.3870 | LR 0.000100\n",
      "Ep 4: TrLoss 0.0925 | Val F1 0.7113 | ValLoss 0.4600 | LR 0.000100\n",
      "Ep 5: TrLoss 0.0868 | Val F1 0.7589 | ValLoss 0.4784 | LR 0.000100\n",
      " Best F1: 0.7589\n",
      "  Testing LR=1e-05, Batch=16...Training on cpu for 5 epochs...\n",
      "Ep 1: TrLoss 0.5173 | Val F1 0.6167 | ValLoss 0.4080 | LR 0.000010\n",
      "Ep 2: TrLoss 0.3747 | Val F1 0.6923 | ValLoss 0.3621 | LR 0.000010\n",
      "Ep 3: TrLoss 0.3100 | Val F1 0.7083 | ValLoss 0.3487 | LR 0.000010\n",
      "Ep 4: TrLoss 0.2642 | Val F1 0.7570 | ValLoss 0.3428 | LR 0.000010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m config = {\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: lr, \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: tune_epochs}\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train (suppress output for cleanliness)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m _, hist = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check result\u001b[39;00m\n\u001b[32m     27\u001b[39m max_f1 = \u001b[38;5;28mmax\u001b[39m(hist[\u001b[33m'\u001b[39m\u001b[33mval_f1\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/src/engine.py:110\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, params, device)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     train_loss, _ = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     val_metrics = validate(model, val_loader, criterion, device)\n\u001b[32m    113\u001b[39m     current_val_f1 = val_metrics[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/src/engine.py:54\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     52\u001b[39m outputs = model(inputs)\n\u001b[32m     53\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m optimizer.step()\n\u001b[32m     57\u001b[39m running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç STARTING HYPERPARAMETER TUNING (on Baseline Data)...\")\n",
    "\n",
    "# Grid to search\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "batch_sizes = [16, 32]\n",
    "tune_epochs = 5  # Short runs to check convergence\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_params = {'lr': 1e-4, 'batch_size': 32}  # Default\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"  Testing LR={lr}, Batch={bs}...\", end=\"\")\n",
    "\n",
    "        # Create loaders\n",
    "        tune_train_loader = get_dataloader(X_train, Y_train, batch_size=bs)\n",
    "        tune_val_loader = get_dataloader(X_val, Y_val, batch_size=bs, shuffle=False)\n",
    "\n",
    "        # Init simple model for tuning\n",
    "        model = ResNet18_Grayscale(num_classes=1).to(device)\n",
    "        config = {'lr': lr, 'epochs': tune_epochs}\n",
    "\n",
    "        # Train (suppress output for cleanliness)\n",
    "        _, hist = train_model(model, tune_train_loader, tune_val_loader, config, device)\n",
    "\n",
    "        # Check result\n",
    "        max_f1 = max(hist['val_f1'])\n",
    "        print(f\" Best F1: {max_f1:.4f}\")\n",
    "\n",
    "        if max_f1 > best_val_f1:\n",
    "            best_val_f1 = max_f1\n",
    "            best_params = {'lr': lr, 'batch_size': bs}\n",
    "\n",
    "print(f\"‚úÖ Tuning Complete. Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4dddde",
   "metadata": {},
   "source": [
    "## Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ STARTING BASELINE TRAINING...\")\n",
    "\n",
    "# Setup\n",
    "final_config = {\n",
    "    'lr': best_params['lr'],\n",
    "    'epochs': 20  # Full training duration\n",
    "}\n",
    "train_loader = get_dataloader(X_train, Y_train, batch_size=best_params['batch_size'])\n",
    "val_loader = get_dataloader(X_val, Y_val, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Train\n",
    "model_base = ResNet18_Grayscale(num_classes=1).to(device)\n",
    "model_base, hist_base = train_model(model_base, train_loader, val_loader, final_config, device)\n",
    "\n",
    "# Save Weights\n",
    "torch.save(model_base.state_dict(), 'results/models/baseline_best.pth')\n",
    "print(\"üíæ Baseline weights saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274cc30",
   "metadata": {},
   "source": [
    "## Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the training history\n",
    "history_filepath = 'results/baseline_training_history.json'\n",
    "\n",
    "# Save the hist_base dictionary to a JSON file\n",
    "with open(history_filepath, 'w') as f:\n",
    "    json.dump(hist_base, f)\n",
    "\n",
    "print(f\"üíæ Training history saved to {history_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b5270",
   "metadata": {},
   "source": [
    "## Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f54f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(hist_base['train_loss']) + 1)\n",
    "\n",
    "# Plotting Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, hist_base['train_loss'], label='Training Loss', color='blue')\n",
    "plt.title('Baseline Model Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "training_loss_filepath = 'results/baseline_training_loss.png'\n",
    "plt.savefig(training_loss_filepath)\n",
    "plt.show()\n",
    "print(f\"üìä Training loss plot saved to {training_loss_filepath}\")\n",
    "\n",
    "# Plotting Validation F1-score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, hist_base['val_f1'], label='Validation F1-score', color='green')\n",
    "plt.title('Baseline Model Validation F1-score over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "validation_f1_filepath = 'results/baseline_validation_f1.png'\n",
    "plt.savefig(validation_f1_filepath)\n",
    "plt.show()\n",
    "print(f\"üìä Validation F1-score plot saved to {validation_f1_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64565d",
   "metadata": {},
   "source": [
    "## Improved Training Configuration\n",
    "\n",
    "### Define Updated Configuration\n",
    "Define an updated configuration dictionary that includes parameters for a learning rate scheduler and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a63f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_config = {\n",
    "    'lr': best_params['lr'],\n",
    "    'epochs': 50,  # Set a sufficiently large number of epochs, as early stopping will manage the actual training duration\n",
    "    'scheduler_patience': 5, # Patience for ReduceLROnPlateau\n",
    "    'scheduler_factor': 0.5, # Factor by which the learning rate will be reduced\n",
    "    'early_stopping_patience': 5, # Number of epochs with no improvement after which training will be stopped\n",
    "    'early_stopping_min_delta': 0.001 # Minimum change to be considered an improvement\n",
    "}\n",
    "\n",
    "print(\"Improved training configuration defined:\")\n",
    "for key, value in improved_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7d110",
   "metadata": {},
   "source": [
    "## Enhanced Engine Module\n",
    "\n",
    "The engine.py module has been updated to include:\n",
    "- EarlyStopping class for preventing overfitting\n",
    "- Learning rate scheduler integration\n",
    "- Enhanced training loop with validation loss tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current engine.py content\n",
    "with open('src/engine.py', 'r') as f:\n",
    "    engine_content = f.read()\n",
    "    print(\"Current engine.py content:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(engine_content[:1000] + \"...\" if len(engine_content) > 1000 else engine_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba689e",
   "metadata": {},
   "source": [
    "## Training with Improvements\n",
    "\n",
    "Train the model with learning rate scheduling and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74298c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ STARTING TRAINING WITH LR SCHEDULER AND EARLY STOPPING...\")\n",
    "\n",
    "# Re-import modules to ensure we have the latest version\n",
    "import importlib\n",
    "import engine\n",
    "importlib.reload(engine)\n",
    "from engine import train_model\n",
    "\n",
    "# Initialize a new model for improved training\n",
    "model_improved = ResNet18_Grayscale(num_classes=1).to(device)\n",
    "\n",
    "# Prepare data loaders using the best batch size from tuning\n",
    "train_loader_improved = get_dataloader(X_train, Y_train, batch_size=best_params['batch_size'])\n",
    "val_loader_improved = get_dataloader(X_val, Y_val, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# Train the model with the improved configuration\n",
    "model_improved, hist_improved = train_model(model_improved, train_loader_improved, val_loader_improved, improved_config, device)\n",
    "\n",
    "# Save the weights of the improved model\n",
    "torch.save(model_improved.state_dict(), 'results/models/improved_best.pth')\n",
    "print(\"üíæ Improved model weights saved to results/models/improved_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d016ea",
   "metadata": {},
   "source": [
    "## Save Improved Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the improved model's training history\n",
    "history_filepath_improved = 'results/improved_training_history.json'\n",
    "\n",
    "# Save the hist_improved dictionary to a JSON file\n",
    "with open(history_filepath_improved, 'w') as f:\n",
    "    json.dump(hist_improved, f)\n",
    "\n",
    "print(f\"üíæ Improved training history saved to {history_filepath_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110891ec",
   "metadata": {},
   "source": [
    "## Visualize Improved Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_improved = range(1, len(hist_improved['train_loss']) + 1)\n",
    "\n",
    "# Plotting Training Loss for Improved Model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_improved, hist_improved['train_loss'], label='Training Loss', color='purple')\n",
    "plt.title('Improved Model Training Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "training_loss_filepath_improved = 'results/improved_training_loss.png'\n",
    "plt.savefig(training_loss_filepath_improved)\n",
    "plt.show()\n",
    "print(f\"üìä Improved training loss plot saved to {training_loss_filepath_improved}\")\n",
    "\n",
    "# Plotting Validation F1-score for Improved Model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_improved, hist_improved['val_f1'], label='Validation F1-score', color='orange')\n",
    "plt.title('Improved Model Validation F1-score over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "validation_f1_filepath_improved = 'results/improved_validation_f1.png'\n",
    "plt.savefig(validation_f1_filepath_improved)\n",
    "plt.show()\n",
    "print(f\"üìä Improved validation F1-score plot saved to {validation_f1_filepath_improved}\")\n",
    "\n",
    "# If validation loss is available, plot it too\n",
    "if 'val_loss' in hist_improved:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_improved, hist_improved['val_loss'], label='Validation Loss', color='red')\n",
    "    plt.title('Improved Model Validation Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    validation_loss_filepath_improved = 'results/improved_validation_loss.png'\n",
    "    plt.savefig(validation_loss_filepath_improved)\n",
    "    plt.show()\n",
    "    print(f\"üìä Improved validation loss plot saved to {validation_loss_filepath_improved}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ba66a",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7b616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "test_loader = get_dataloader(X_test, Y_test, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"\\nüìä EVALUATING MODELS ON TEST SET...\")\n",
    "\n",
    "# Evaluate baseline model\n",
    "from engine import validate\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"\\nBaseline Model Test Results:\")\n",
    "baseline_test_metrics = validate(model_base, test_loader, criterion, device)\n",
    "for metric, value in baseline_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nImproved Model Test Results:\")\n",
    "improved_test_metrics = validate(model_improved, test_loader, criterion, device)\n",
    "for metric, value in improved_test_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'baseline': baseline_test_metrics,\n",
    "    'improved': improved_test_metrics,\n",
    "    'best_params': best_params\n",
    "}\n",
    "\n",
    "with open('results/test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Test results saved to results/test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118bd1c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Complete!\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Found optimal learning rate and batch size\n",
    "2. **Baseline Training**: Trained a simple ResNet18 model\n",
    "3. **Enhanced Training**: Implemented learning rate scheduling and early stopping\n",
    "4. **Model Evaluation**: Compared both models on the test set\n",
    "5. **Results Saved**: All models, histories, and plots saved to `results/` directory\n",
    "\n",
    "### Files Generated:\n",
    "- `results/models/baseline_best.pth` - Baseline model weights\n",
    "- `results/models/improved_best.pth` - Improved model weights\n",
    "- `results/baseline_training_history.json` - Baseline training history\n",
    "- `results/improved_training_history.json` - Improved training history\n",
    "- `results/test_results.json` - Test set evaluation results\n",
    "- Various plots in `results/` directory\n",
    "\n",
    "The improved model with learning rate scheduling and early stopping should show better convergence and potentially better performance than the baseline model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
