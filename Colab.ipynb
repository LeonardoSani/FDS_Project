{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5378081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import src.pre_processing as pp\n",
    "import src.ResNet18 as rn\n",
    "import src.engine as eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c16cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and split \n",
    "X, Y, Z, proba = pp.get_data()\n",
    "X_train, Y_train, Z_train, X_val, Y_val, Z_val, X_test, Y_test, Z_test = pp.split_data(X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eac910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "--- Starting Hyperparameter Tuning (5 trials) ---\n",
      "\n",
      "[Trial 1/5] Params: {'lr': 0.001, 'batch_size': 16, 'weight_decay': 0, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6884 (Loss: 0.4764)\n",
      "   *** New Best Model Found! ***\n",
      "\n",
      "[Trial 2/5] Params: {'lr': 0.0001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6884 (Loss: 0.4764)\n",
      "   *** New Best Model Found! ***\n",
      "\n",
      "[Trial 2/5] Params: {'lr': 0.0001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.7491 (Loss: 0.3590)\n",
      "   *** New Best Model Found! ***\n",
      "\n",
      "[Trial 3/5] Params: {'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.7491 (Loss: 0.3590)\n",
      "   *** New Best Model Found! ***\n",
      "\n",
      "[Trial 3/5] Params: {'lr': 0.001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6842 (Loss: 0.4308)\n",
      "\n",
      "[Trial 4/5] Params: {'lr': 0.001, 'batch_size': 16, 'weight_decay': 0, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6842 (Loss: 0.4308)\n",
      "\n",
      "[Trial 4/5] Params: {'lr': 0.001, 'batch_size': 16, 'weight_decay': 0, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6455 (Loss: 0.5723)\n",
      "\n",
      "[Trial 5/5] Params: {'lr': 5e-05, 'batch_size': 64, 'weight_decay': 0.0001, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.6455 (Loss: 0.5723)\n",
      "\n",
      "[Trial 5/5] Params: {'lr': 5e-05, 'batch_size': 64, 'weight_decay': 0.0001, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.7067 (Loss: 0.4586)\n",
      "\n",
      "--- Tuning Complete ---\n",
      "Best F1 Score: 0.7491\n",
      "Best Params: {'lr': 0.0001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n",
      "   -> Result: Val F1: 0.7067 (Loss: 0.4586)\n",
      "\n",
      "--- Tuning Complete ---\n",
      "Best F1 Score: 0.7491\n",
      "Best Params: {'lr': 0.0001, 'batch_size': 16, 'weight_decay': 1e-05, 'epochs': 3}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],   # Added intermediate steps and lower bound\n",
    "    'batch_size': [16, 32, 64, 128],        # Added larger batch size\n",
    "    'weight_decay': [1e-3, 1e-4, 1e-5, 0],  # Added stronger regularization\n",
    "    'epochs': [3] \n",
    "}\n",
    "\n",
    "best_params = eng.tune_hyperparameters(rn.ResNet18_Grayscale, X_train, Y_train, X_val, Y_val, param_grid, device, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693adbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = best_params['batch_size']\n",
    "train_loader = eng.create_loader(X_train, Y_train, BATCH_SIZE)\n",
    "val_loader = eng.create_loader(X_val, Y_val, BATCH_SIZE)\n",
    "test_loader = eng.create_loader(X_test, Y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667090d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on mps for 20 epochs...\n",
      "Ep 1: TrLoss 0.4484 | Val F1 0.7124 | ValLoss 0.3768 | LR 0.000100\n",
      "Ep 1: TrLoss 0.4484 | Val F1 0.7124 | ValLoss 0.3768 | LR 0.000100\n",
      "Ep 2: TrLoss 0.2975 | Val F1 0.7404 | ValLoss 0.3642 | LR 0.000100\n",
      "Ep 2: TrLoss 0.2975 | Val F1 0.7404 | ValLoss 0.3642 | LR 0.000100\n",
      "Ep 3: TrLoss 0.2556 | Val F1 0.7373 | ValLoss 0.4044 | LR 0.000100\n",
      "Ep 3: TrLoss 0.2556 | Val F1 0.7373 | ValLoss 0.4044 | LR 0.000100\n",
      "Ep 4: TrLoss 0.1912 | Val F1 0.7372 | ValLoss 0.3914 | LR 0.000100\n",
      "Ep 4: TrLoss 0.1912 | Val F1 0.7372 | ValLoss 0.3914 | LR 0.000100\n",
      "Ep 5: TrLoss 0.1429 | Val F1 0.7008 | ValLoss 0.5018 | LR 0.000100\n",
      "Ep 5: TrLoss 0.1429 | Val F1 0.7008 | ValLoss 0.5018 | LR 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m final_params[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m20\u001b[39m\n\u001b[32m      5\u001b[39m final_params[\u001b[33m'\u001b[39m\u001b[33mearly_stopping_patience\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model, history, best_metrics = \u001b[43meng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/src/engine.py:118\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, params, device, verbose)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     train_loss, _ = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     val_metrics = validate(model, val_loader, criterion, device)\n\u001b[32m    121\u001b[39m     current_val_f1 = val_metrics[\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/src/engine.py:56\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     54\u001b[39m outputs = model(inputs)\n\u001b[32m     55\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m optimizer.step()\n\u001b[32m     59\u001b[39m running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/FDS_project2/FDS_Project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = rn.ResNet18_Grayscale().to(device)\n",
    "\n",
    "final_params = best_params.copy()\n",
    "final_params['epochs'] = 20\n",
    "final_params['early_stopping_patience'] = 5\n",
    "\n",
    "model, history, best_metrics = eng.train_model(model, train_loader, val_loader, final_params, device)\n",
    "\n",
    "# Save Baseline Model and History\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.makedirs('results/models', exist_ok=True)\n",
    "torch.save(model.state_dict(), 'results/models/model_baseline.pth')\n",
    "pd.DataFrame(history).to_csv('results/history_baseline.csv', index=False)\n",
    "print(\"Baseline model and history saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c53615",
   "metadata": {},
   "source": [
    "# Experimenting with the Augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment with Geometric Augmentation ---\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create Geometrically Augmented Dataset\n",
    "# We augment the minority class to balance the dataset\n",
    "X_train_aug, Y_train_aug, Z_train_aug = pp.augment_data(X_train, Y_train, Z_train)\n",
    "\n",
    "# Combine original and augmented data\n",
    "X_train_geo = np.concatenate((X_train, X_train_aug), axis=0)\n",
    "Y_train_geo = np.concatenate((Y_train, Y_train_aug), axis=0)\n",
    "Z_train_geo = np.concatenate((Z_train, Z_train_aug), axis=0)\n",
    "\n",
    "print(f\"Training with Geometric Augmentation. New dataset size: {len(X_train_geo)}\")\n",
    "\n",
    "# 2. Tune Hyperparameters on the augmented dataset\n",
    "best_params_geo = eng.tune_hyperparameters(rn.ResNet18_Grayscale, X_train_geo, Y_train_geo, X_val, Y_val, param_grid, device, n_trials=5)\n",
    "\n",
    "# 3. Create Loaders using the new best batch size\n",
    "BATCH_SIZE_GEO = best_params_geo['batch_size']\n",
    "train_loader_geo = eng.create_loader(X_train_geo, Y_train_geo, BATCH_SIZE_GEO)\n",
    "val_loader_geo = eng.create_loader(X_val, Y_val, BATCH_SIZE_GEO)\n",
    "test_loader_geo = eng.create_loader(X_test, Y_test, BATCH_SIZE_GEO)\n",
    "\n",
    "# 4. Train Model with best parameters\n",
    "model_geo = rn.ResNet18_Grayscale().to(device)\n",
    "\n",
    "final_params_geo = best_params_geo.copy()\n",
    "final_params_geo['epochs'] = 20\n",
    "final_params_geo['early_stopping_patience'] = 5\n",
    "\n",
    "model_geo, history_geo, best_metrics_geo = eng.train_model(model_geo, train_loader_geo, val_loader_geo, final_params_geo, device)\n",
    "\n",
    "# Save Geometric Model and History\n",
    "torch.save(model_geo.state_dict(), 'results/models/model_geo.pth')\n",
    "pd.DataFrame(history_geo).to_csv('results/history_geo.csv', index=False)\n",
    "print(\"Geometric model and history saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deddc38a",
   "metadata": {},
   "source": [
    "# Experimenting with VAE Generated Dataset\n",
    "This section loads a pre-trained VAE model to generate synthetic defect samples and augments the training set with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5cf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment with VAE Generated Dataset ---\n",
    "import src.VaeA as vA\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load VAE Model\n",
    "# TODO: Download the weights file and set the correct path here\n",
    "weights_path = 'vaeA_model_Beta_ann_weights.pth' \n",
    "\n",
    "vae_model = vA.VAE(latent_dim=128, img_channels=1)\n",
    "\n",
    "# Load weights if available\n",
    "if os.path.exists(weights_path):\n",
    "    vae_model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    vae_model.to(device)\n",
    "    vae_model.eval()\n",
    "    print(\"VAE model loaded successfully.\")\n",
    "    \n",
    "    # 2. Prepare Data for Generation\n",
    "    # We use the geometrically augmented dataset as the source for the VAE to learn/sample from\n",
    "    \n",
    "    X_train_vae = pp.get_defect(X_train_geo, Y_train_geo)\n",
    "    # Manually filter Z for defected samples as pp.get_defect only returns X\n",
    "    Z_train_vae = Z_train_geo[Y_train_geo == 1]\n",
    "    \n",
    "    X_healthy = X_train_geo[Y_train_geo == 0]\n",
    "\n",
    "    # 3. Generate Samples\n",
    "    print(\"Generating samples with VAE...\")\n",
    "    # Note: Adjust n_samples as needed. Notebook.ipynb used 800.\n",
    "    X_gen = vA.generate_controlled_samples(vae_model, X_train_vae, Z_train_vae, X_healthy, n_samples=800, mode='spherical', threshold=7.0, device=device)\n",
    "    Y_gen = np.ones(X_gen.shape[0])\n",
    "    \n",
    "    print(f\"Generated {len(X_gen)} samples.\")\n",
    "\n",
    "    # 4. Create VAE Augmented Dataset (Original + Generated)\n",
    "    X_train_vae_aug = np.concatenate((X_train, X_gen), axis=0)\n",
    "    Y_train_vae_aug = np.concatenate((Y_train, Y_gen), axis=0)\n",
    "    \n",
    "    print(f\"Training with VAE Augmentation. New dataset size: {len(X_train_vae_aug)}\")\n",
    "\n",
    "    # 5. Tune Hyperparameters on the VAE augmented dataset\n",
    "    best_params_vae = eng.tune_hyperparameters(rn.ResNet18_Grayscale, X_train_vae_aug, Y_train_vae_aug, X_val, Y_val, param_grid, device, n_trials=5)\n",
    "\n",
    "    # 6. Create Loaders\n",
    "    BATCH_SIZE_VAE = best_params_vae['batch_size']\n",
    "    train_loader_vae = eng.create_loader(X_train_vae_aug, Y_train_vae_aug, BATCH_SIZE_VAE)\n",
    "    val_loader_vae = eng.create_loader(X_val, Y_val, BATCH_SIZE_VAE)\n",
    "    test_loader_vae = eng.create_loader(X_test, Y_test, BATCH_SIZE_VAE)\n",
    "\n",
    "    # 7. Train Model\n",
    "    model_vae = rn.ResNet18_Grayscale().to(device)\n",
    "    \n",
    "    final_params_vae = best_params_vae.copy()\n",
    "    final_params_vae['epochs'] = 20\n",
    "    final_params_vae['early_stopping_patience'] = 5\n",
    "\n",
    "    model_vae, history_vae, best_metrics_vae = eng.train_model(model_vae, train_loader_vae, val_loader_vae, final_params_vae, device)\n",
    "\n",
    "    # Save VAE Model and History\n",
    "    torch.save(model_vae.state_dict(), 'results/models/model_vae.pth')\n",
    "    pd.DataFrame(history_vae).to_csv('results/history_vae.csv', index=False)\n",
    "    print(\"VAE model and history saved.\")\n",
    "\n",
    "else:\n",
    "    print(f\"WARNING: Weights file not found at {weights_path}. Please download the weights and update the path to proceed with generation and training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparative Analysis ---\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Histories\n",
    "# Assuming files exist if previous cells ran successfully\n",
    "history_baseline = pd.read_csv('results/history_baseline.csv')\n",
    "history_geo = pd.read_csv('results/history_geo.csv')\n",
    "\n",
    "histories = {\n",
    "    'Baseline': history_baseline,\n",
    "    'Geometric': history_geo\n",
    "}\n",
    "\n",
    "# Check if VAE history exists (since it's conditional)\n",
    "if os.path.exists('results/history_vae.csv'):\n",
    "    history_vae = pd.read_csv('results/history_vae.csv')\n",
    "    histories['VAE'] = history_vae\n",
    "\n",
    "# 2. Plot Comparative Curves\n",
    "if histories:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Training Loss\n",
    "    for name, hist in histories.items():\n",
    "        axes[0].plot(hist['train_loss'], label=name)\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Validation Loss\n",
    "    for name, hist in histories.items():\n",
    "        axes[1].plot(hist['val_loss'], label=name)\n",
    "    axes[1].set_title('Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Validation F1 Score\n",
    "    for name, hist in histories.items():\n",
    "        axes[2].plot(hist['val_f1'], label=name)\n",
    "    axes[2].set_title('Validation F1 Score')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('F1 Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Evaluate Models on Test Set\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(labels.numpy())\n",
    "            \n",
    "    return np.array(all_targets), np.array(all_preds)\n",
    "\n",
    "models_to_eval = {}\n",
    "if 'model' in locals(): models_to_eval['Baseline'] = model\n",
    "if 'model_geo' in locals(): models_to_eval['Geometric'] = model_geo\n",
    "if 'model_vae' in locals(): models_to_eval['VAE'] = model_vae\n",
    "\n",
    "# If models are not in locals, try loading them\n",
    "if not models_to_eval:\n",
    "    print(\"Models not found in memory. Attempting to load from disk...\")\n",
    "    if os.path.exists('results/models/model_baseline.pth'):\n",
    "        m = rn.ResNet18_Grayscale().to(device)\n",
    "        m.load_state_dict(torch.load('results/models/model_baseline.pth', map_location=device))\n",
    "        models_to_eval['Baseline'] = m\n",
    "    \n",
    "    if os.path.exists('results/models/model_geo.pth'):\n",
    "        m = rn.ResNet18_Grayscale().to(device)\n",
    "        m.load_state_dict(torch.load('results/models/model_geo.pth', map_location=device))\n",
    "        models_to_eval['Geometric'] = m\n",
    "        \n",
    "    if os.path.exists('results/models/model_vae.pth'):\n",
    "        m = rn.ResNet18_Grayscale().to(device)\n",
    "        m.load_state_dict(torch.load('results/models/model_vae.pth', map_location=device))\n",
    "        models_to_eval['VAE'] = m\n",
    "\n",
    "# 4. Comparative Table and Confusion Matrices\n",
    "results_table = []\n",
    "\n",
    "if models_to_eval:\n",
    "    fig, axes = plt.subplots(1, len(models_to_eval), figsize=(5 * len(models_to_eval), 4))\n",
    "    if len(models_to_eval) == 1: axes = [axes]\n",
    "    \n",
    "    for i, (name, model_obj) in enumerate(models_to_eval.items()):\n",
    "        targets, preds = evaluate_model(model_obj, test_loader, device)\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(targets, preds)\n",
    "        f1 = f1_score(targets, preds)\n",
    "        prec = precision_score(targets, preds)\n",
    "        rec = recall_score(targets, preds)\n",
    "        \n",
    "        results_table.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': acc,\n",
    "            'F1 Score': f1,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec\n",
    "        })\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(targets, preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "        axes[i].set_title(f'{name} Confusion Matrix')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display Table\n",
    "    results_df = pd.DataFrame(results_table)\n",
    "    print(\"\\nComparative Results:\")\n",
    "    print(results_df)\n",
    "else:\n",
    "    print(\"No models available for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fds-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
